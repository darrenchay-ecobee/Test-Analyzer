{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for data retrieval\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Importing libraries for data processing\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "\n",
    "if TOKEN is None:\n",
    "    raise ValueError(\"TOKEN environment variable is not set. Please set it in the .env file.\")\n",
    "\n",
    "OWNER = os.getenv(\"OWNER\")\n",
    "REPO = os.getenv(\"REPO\")\n",
    "START_TIME = os.getenv(\"START_TIME\")\n",
    "END_TIME = os.getenv(\"END_TIME\")\n",
    "DESTINATION_DIR_ZIPS = os.getenv(\"DESTINATION_DIR_ZIPS\")\n",
    "DESTINATION_DIR_DATA = os.getenv(\"DESTINATION_DIR_DATA\")\n",
    "WORKFLOW_ID = os.getenv(\"WORKFLOW_ID\")\n",
    "\n",
    "# Create the destination directories if they don't exist\n",
    "os.makedirs(DESTINATION_DIR_ZIPS, exist_ok=True)\n",
    "os.makedirs(DESTINATION_DIR_DATA, exist_ok=True)\n",
    "\n",
    "# Ensure the destination directories are writable\n",
    "os.chmod(DESTINATION_DIR_ZIPS, 0o777)\n",
    "os.chmod(DESTINATION_DIR_DATA, 0o777)\n",
    "\n",
    "# Function to fetch all workflow runs and handle pagination\n",
    "def fetch_runs(url, headers):\n",
    "    all_run_ids = []\n",
    "    while url:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Filter runs by created_at time range\n",
    "        runs = [run['id'] for run in data['workflow_runs']\n",
    "                    if START_TIME <= run['created_at'] <= END_TIME \n",
    "                    # and run['head_branch'] == 'main'\n",
    "                ]\n",
    "        all_run_ids.extend(runs)\n",
    "\n",
    "        # Check for pagination\n",
    "        url = None\n",
    "        if 'next' in response.links:\n",
    "            url = response.links['next']['url']\n",
    "            print(\"retrieving from\", url, \"num runs: \", len(all_run_ids))\n",
    "\n",
    "    return all_run_ids\n",
    "\n",
    "# Fetch all workflow runs for the specified workflow\n",
    "url = f\"https://api.github.com/repos/{OWNER}/{REPO}/actions/workflows/{WORKFLOW_ID}/runs\"\n",
    "headers = {\"Authorization\": f\"token {TOKEN}\"}\n",
    "\n",
    "# Extract run IDs from the JSON response, filtering by the time range\n",
    "run_ids = fetch_runs(url, headers)\n",
    "\n",
    "# Check if any runs are found in the time range\n",
    "if not run_ids:\n",
    "    print(\"No runs found in the specified time range.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data\n",
    "def parse_test_report(file_path: str) -> ET.Element:\n",
    "    tree = ET.parse(file_path)\n",
    "    return tree.getroot()\n",
    "\n",
    "# Create a test data object for each file\n",
    "def extract_test_data(root: ET.Element, test_type: str) -> list:\n",
    "    test_data = []\n",
    "    for testcase in root.findall('testcase'):\n",
    "        name = testcase.attrib.get('name')\n",
    "        classname = testcase.attrib.get('classname')\n",
    "        time = float(testcase.attrib.get('time', 0))\n",
    "        failure = testcase.find('failure') is not None\n",
    "        error = testcase.find('error') is not None\n",
    "        test_data.append({\n",
    "            'name': name,\n",
    "            'classname': classname,\n",
    "            'test_type': test_type,\n",
    "            'time': time,\n",
    "            'failure': failure,\n",
    "            'error': error\n",
    "        })\n",
    "    return test_data\n",
    "\n",
    "# Extracts the data from all files and returns in list form for analysis\n",
    "def collect_test_data(report_dir: str, test_type: str) -> list:\n",
    "    test_data = []\n",
    "    for root, _, files in os.walk(report_dir):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.xml'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                root_element = parse_test_report(file_path)\n",
    "                data = extract_test_data(root_element, test_type)\n",
    "                test_data.append(data)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving data\n",
    "test_data = []\n",
    "\n",
    "# Loop through each workflow run\n",
    "for run_id in run_ids:\n",
    "    print(f\"Downloading artifacts from run {run_id}...\")\n",
    "\n",
    "    # Get artifacts for the current run\n",
    "    url = f\"https://api.github.com/repos/{OWNER}/{REPO}/actions/runs/{run_id}/artifacts\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch URL: {url}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract artifact URLs and names from the JSON response\n",
    "    artifacts = response.json()[\"artifacts\"]\n",
    "    for artifact in artifacts:\n",
    "        if \"Test Results\" in artifact[\"name\"]:\n",
    "            artifact_name = artifact[\"name\"]\n",
    "            artifact_url = artifact[\"archive_download_url\"]\n",
    "\n",
    "            # Download the artifact to the zips folder\n",
    "            filename = f\"{artifact_name}.zip\"\n",
    "            zip_path = os.path.join(DESTINATION_DIR_ZIPS, filename)\n",
    "            response = requests.get(artifact_url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch artifact: {artifact_url} ignoring the other artifacts in this run\")\n",
    "                break\n",
    "            # response.raise_for_status()  # Raise an exception for failed requests\n",
    "            with open(zip_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            # Create a folder for unzipped artifact\n",
    "            artifact_dir = os.path.join(DESTINATION_DIR_DATA, artifact_name)\n",
    "            os.makedirs(artifact_dir, exist_ok=True)\n",
    "\n",
    "            # Ensure the directory is writable\n",
    "            os.chmod(artifact_dir, 0o777)\n",
    "\n",
    "            # Unzip the artifact\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(artifact_dir)\n",
    "\n",
    "            # Load data to list\n",
    "            test_data.append(collect_test_data(report_dir=\"./temp\", test_type=artifact_name[:30]))\n",
    "            \n",
    "            # Delete the zip and extracted files after unzipping to prevent storage from filling up\n",
    "            os.remove(zip_path)\n",
    "            shutil.rmtree(artifact_dir)\n",
    "\n",
    "print(\"All artifacts downloaded and extracted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the data could be saved in a storage bucket so we could analyze it for longer periods since the retention seems to be around 2 weeks only \n",
    "- upload the zips to gcs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the relevant info from the test data and creates a dataframe\n",
    "def analyze_test_data(test_data: list) -> pd.DataFrame:\n",
    "    test_runs = defaultdict(lambda: {'test_class': '','runs': 0, 'failures': 0, 'total_time': 0.0, 'test_type': ''})\n",
    "\n",
    "    for run_data in test_data:\n",
    "        for test in run_data:\n",
    "            test_name = f\"{test['name']}\"\n",
    "            test_runs[test_name]['runs'] += 1\n",
    "            test_runs[test_name]['test_class'] = test['classname']\n",
    "            test_runs[test_name]['test_type'] = test['test_type']\n",
    "            test_runs[test_name]['total_time'] += test['time']\n",
    "            if test['failure'] or test['error']:\n",
    "                test_runs[test_name]['failures'] += 1\n",
    "\n",
    "    # Create a DataFrame\n",
    "    test_data_df = pd.DataFrame([\n",
    "        {'test_name': test_name, 'test_class': counts['test_class'], 'test_type': counts['test_type'], 'runs': counts['runs'], 'failures': counts['failures'], 'total_time': counts['total_time']}\n",
    "        for test_name, counts in test_runs.items()\n",
    "    ])\n",
    "\n",
    "    # Calculate failure percentage for each test\n",
    "    test_data_df['failure_percentage'] = (test_data_df['failures'] / test_data_df['runs']) * 100\n",
    "\n",
    "    # Threshold percentage for flakiness\n",
    "    threshold_percentage = 5  # Adjust as needed\n",
    "\n",
    "    # Mark a test as flakey if failure percentage is > threshold\n",
    "    test_data_df['flakey'] = test_data_df['failure_percentage'] > threshold_percentage\n",
    "    test_data_df['avg_time'] = test_data_df['total_time'] / test_data_df['runs']\n",
    "\n",
    "    return test_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing and analyzing data\n",
    "all_test_data = []\n",
    "for test_data_run in test_data:\n",
    "    all_test_data.extend(test_data_run)\n",
    "    \n",
    "\n",
    "test_data_df = analyze_test_data(all_test_data)\n",
    "# Display the DataFrame\n",
    "top_failures_tests = test_data_df.sort_values(by='runs', ascending=False).head(50)\n",
    "\n",
    "display(top_failures_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing all parts of repo which contains tests\n",
    "def get_class_mapping(row):\n",
    "    classname = row['test_class'].split('.')[2]\n",
    "    return classname\n",
    "test_data_df['class_location'] = test_data_df.apply(get_class_mapping, axis=1)\n",
    "display(test_data_df['class_location'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding github link for each test\n",
    "\n",
    "folder_class_map = { # Did some for now\n",
    "    \"foundation\": \"foundation/test/source\",\n",
    "    # \"cache\": \"lib-cache\",\n",
    "    \"communicator\": \"communicator/test/source\",\n",
    "    \"ests\": \"ests-pubsub/src/test/java\",\n",
    "    \"events\": \"lib-events/test/java\",\n",
    "    \"reportprocessor\": \"report-processor/src/test/java\",\n",
    "    \"webapp\": \"gui/test/src\",\n",
    "    \"contractorservice\": \"libs/communicator/contractor-service-client\",\n",
    "}\n",
    "\n",
    "# Base URL for the GitHub repository\n",
    "base_github_url = f\"https://github.com/{OWNER}/{REPO}/tree/main\"\n",
    "\n",
    "def create_github_link(row):\n",
    "    location = row['test_class'].split('.')[2]\n",
    "    # Replace dots in the class name with slashes to form the path\n",
    "    class_path = row['test_class'].replace('.', '/')\n",
    "    # Construct the GitHub URL\n",
    "    return f\"{base_github_url}/{folder_class_map.get(location)}/{class_path}.java\"\n",
    "\n",
    "# Add a column for GitHub links \n",
    "test_data_df['github_link'] = test_data_df.apply(create_github_link, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "Not all github links will work, just did the quickest approach to get the majority of links for ease of searching up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_time_tests = test_data_df.sort_values(by='avg_time', ascending=False).head(50)\n",
    "\n",
    "display(top_time_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold percentage for flakiness\n",
    "threshold_percentage = 1  # Adjust as needed\n",
    "\n",
    "# Mark a test as flakey if failure percentage is > threshold\n",
    "test_data_df['flakey'] = test_data_df['failure_percentage'] > threshold_percentage\n",
    "\n",
    "flakey_tests = test_data_df[test_data_df['flakey']].sort_values(by='failure_percentage', ascending=False)\n",
    "display(flakey_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = test_data_df.sort_values(by='runs', ascending=False).head(50)\n",
    "\n",
    "display(num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of flakey tests per class_location\n",
    "flakey_counts = test_data_df.groupby('class_location')['flakey'].sum().reset_index(name='flakey_count')\n",
    "total_counts = test_data_df.groupby('class_location')['flakey'].count().reset_index(name='total_count')\n",
    "\n",
    "flakey_percentage_per_class = pd.merge(flakey_counts, total_counts, on='class_location')\n",
    "flakey_percentage_per_class['flakey_percentage'] = (flakey_percentage_per_class['flakey_count'] / flakey_percentage_per_class['total_count']) * 100\n",
    "\n",
    "# Calculate the average time per class_location\n",
    "avg_time_per_class = test_data_df.groupby('class_location')['avg_time'].mean().reset_index()\n",
    "\n",
    "# Select the top 10 class locations with the highest average time\n",
    "top_avg_time = avg_time_per_class.sort_values('avg_time', ascending=False)\n",
    "\n",
    "\n",
    "# Merge the two DataFrames\n",
    "class_metrics_df = pd.merge(flakey_percentage_per_class, top_avg_time, on='class_location')\n",
    "\n",
    "# Extract the first test_name for each class_location\n",
    "first_test_name = test_data_df.groupby('class_location')['test_type'].first().reset_index()\n",
    "\n",
    "# Merge to get the first test name\n",
    "class_metrics_df = pd.merge(class_metrics_df, first_test_name, on='class_location')\n",
    "\n",
    "# Add a new column for the concatenated label\n",
    "class_metrics_df['label'] = class_metrics_df['class_location'] + ' - ' + class_metrics_df['test_type']\n",
    "display(class_metrics_df)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot flakey percentage\n",
    "sns.barplot(x='flakey_count', y='label', data=class_metrics_df.sort_values('flakey_percentage', ascending=False), ax=ax[0])\n",
    "ax[0].set_title('Class Locations with Highest Count of Flakey Tests')\n",
    "ax[0].set_xlabel('Flakey Count')\n",
    "ax[0].set_ylabel('Class Location and Test Type')\n",
    "\n",
    "# Plot average time\n",
    "sns.barplot(x='avg_time', y='label', data=class_metrics_df.sort_values('avg_time', ascending=False), ax=ax[1])\n",
    "ax[1].set_title('Top 10 Class Locations with Highest Average Time')\n",
    "ax[1].set_xlabel('Average Time (s)')\n",
    "ax[1].set_ylabel('Class Location and Test Name')\n",
    "\n",
    "# Rotate class location labels for better readability\n",
    "for axis in ax:\n",
    "    for label in axis.get_yticklabels():\n",
    "        label.set_rotation(0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above is just a quick attempt to see if there is a way to graph the data and see which areas are the biggest pain points right now in terms of time and flakeyness (probably needs to be better processed)\n",
    "\n",
    "Looking at the data though, it looks like report processor has a lot of flakey tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the average time per class_location\n",
    "# failures_per_class = test_data_df.groupby('class_location')['failure_percentage'].mean().reset_index()\n",
    "\n",
    "# # Select the top 10 class locations with the highest average time\n",
    "# top_failure_time = failures_per_class.sort_values('failure_percentage', ascending=False).head(10)\n",
    "# display(top_avg_time)\n",
    "\n",
    "# # Extract the first test_name for each class_location\n",
    "# first_test_name = test_data_df.groupby('class_location')['test_type'].first().reset_index()\n",
    "\n",
    "# # Merge to get the first test name\n",
    "# top_failure_time = pd.merge(top_failure_time, first_test_name, on='class_location')\n",
    "\n",
    "# # Add a new column for the concatenated label\n",
    "# top_failure_time['label'] = top_failure_time['class_location'] + ' - ' + top_failure_time['test_type']\n",
    "\n",
    "# # Plotting\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "# # Plot flakey percentage\n",
    "# sns.barplot(x='failure_percentage', y='label', data=top_failure_time.sort_values('failure_percentage', ascending=False))\n",
    "# ax[0].set_title('Class Locations with Highest Percentage of failures')\n",
    "# ax[0].set_xlabel('Failure Percentage')\n",
    "# ax[0].set_ylabel('Class Location and Test Type')\n",
    "\n",
    "# # Rotate class location labels for better readability\n",
    "# for axis in ax:\n",
    "#     for label in axis.get_yticklabels():\n",
    "#         label.set_rotation(0)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportprocessor_data = test_data_df[test_data_df[\"class_location\"] == \"reportprocessor\"]\n",
    "\n",
    "display(reportprocessor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_data = test_data_df[(test_data_df[\"class_location\"] == \"foundation\") & (test_data_df[\"flakey\"] == True)]\n",
    "\n",
    "display(foundation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communicator_data = test_data_df[(test_data_df[\"class_location\"] == \"communicator\") & (test_data_df[\"flakey\"] == True)]\n",
    "\n",
    "display(communicator_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "test_data_df.sort_values(by='avg_time', ascending=False).to_csv('test_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
